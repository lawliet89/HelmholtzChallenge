\documentclass{article}
\usepackage{tabularx}

\date{}
\title{Coursework 2 Report}
\author{\\
  \small{Oskar Weigl (ow610)}\\
  \small{Ryan Savitski (rs5010)}\\
  \small{Yong Wen Chua (ywc110)}
}

\begin{document}
\maketitle
\section{General Approach}

The general approach taken to speed up the execution of the calculation is to exploit the parallelism inherent in the problem itself. In particular, by studying the source code, we can see that for each of the wrapper function calls, the cells calculations are mostly independent of each other. The cell calculations can therefore be made to run in parallel. Due to unstructured nature of the mesh, calculations over the entire space must be complete before the next call to the next wrapper function can be made.

In each of the cell, calculation is written to a buffer in which each element of the buffer corresponds to one vertex in the mesh. Since a vertex can belong to more than one cell, care must be taken to ensure that the values accumulated in each vertex is done so without any race conditions such as an atomic add.

The unstructured nature of the mesh also means that trying to exploit spatial locality might be difficult. It will also be difficult to divide the entire iteration space into independent blocks because the memory locations are not ``adjacent'' to each other. 

\section{Threading}

The first approach to speeding up the calculation was to use threads to exploit the parallelism inherent in the problem. Using the Threading Building Blocks library\footnote{https://www.threadingbuildingblocks.org/}, the iteration space was made into what is essentially a parallel for loop that the library can schedule to run in parallel using threads depending on the hardware available. 

There are two avenues for parallerise the work being done: over each ``column'' of cells or over each cell individually.  We hypothesise that while both variants of threading will bring about performance improvements to the computation over the original serialised version, the former parallel version is expected to work better than the latter. This is because the work done per cell might be too small compared to the overheads of thread scheduling and context switching. 

The results shown in table \ref{table:threads} confirms our hypothesis. We notice that threading by columns gives the largest amount of speed up over the original version, by approximately a factor of four. For threading by cell, the performance for most operations are improved significantly, though not as much as threading by columns. Finally, interpolating expressions became slower in this version because of the overheads in thread scheduling when each thread does too little work.

\section{CUDA Acceleration}

one thread per cell -> adjacent threads in blocks are staked vertically

Checking disassembly, the constant matricies used in wrap_lhs_GPU were stored on the stack, in local memory. These were then accessed in the hot loop.
Moving constant data from the kernel to constant data memory improved execution time from 154ms to 64ms for 1.5M threads total (for 1/20th of the full small mesh, for testing)
From IPC 0.4 to 0.77.
Local memory from 133Mreq, 19GB to 61Mreq, 8GB.
In both cases we get 125GB/s, so we have a good indication that local memory may still be the bottleneck.
For comaprison, Global memory is currently 3.3Mreq/s.

Before transpose 70ms for 1/20, 116 after: transpose made it worse for some reason!

For RHS: 114ms before constant memory.

After basically only constant memory optimisations, we have total times:
CPU: 72.8s breakdown: (eval: 6.90, interp: 2.58, RHS: 13.4, LHS: 46.3)
GPU: 2.58s

Final GPU:
Total: 2.23 seconds. See timeline for breakdown. (27ms, 160ms, 715ms, 980ms)
Serialisation 57\%


\section{Summary Table}
\subsection{Threading Performance}
Testing was done on a laptop with Intel Core i7-4700MQ with 4 cores running on Linux 13.10. The programs were compiled with GCC with optimisation set to level 3. The results are summarised in the table below, and the times are consistent across several runs with very minor variations. The times for the steps ``Set array to zero'' and ``Evaluating expression'' are not reported because they are trivial cases that has been optimised by the compiler. The timings were generated from the large mesh.

\begin{table}[h]
\begin{tabularx}{1.2\textwidth}{ |X|X|X|X|X| }
\cline{2-5}
                                          & Evaluating Exp & Interpolating Exp & Assembling RHS & Assembling LHS \\ \hline
\multicolumn{1}{|l|}{Original Version}    & 40.0307        & 4.86542           & 21.0748        & 37.747         \\ \hline
\multicolumn{1}{|l|}{Threading by Cell}   & 12.993         & 5.59316           & 11.0813        & 14.9041        \\ \hline
\multicolumn{1}{|l|}{Threading by Column} & 7.77536        & 1.19127           & 5.64956        & 9.59484        \\ \hline
\end{tabularx}

\caption{Timings in seconds for threaded version using the large data mesh.}
\label{table:threads}
\end{table}
\subsection{CUDA Performance}

\end{document}
